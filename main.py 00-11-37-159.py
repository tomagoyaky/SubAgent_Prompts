import requests
import json
import time
from typing import List, Dict, Optional, Generator
from skills_loader import list_skills, format_skills_system_prompt, get_skill_content, match_skills_by_tags, SkillMetadata

# ===================== å…¨å±€é…ç½® =====================
# æ¨¡å‹é…ç½®ï¼ˆæ”¯æŒåˆ‡æ¢DeepSeek/Ollamaï¼‰
MODEL_TYPE = "deepseek"  # å¯é€‰ "ollama" æˆ– "deepseek"
# MODEL_TYPE = "ollama"  # å¯é€‰ "ollama" æˆ– "deepseek"
DEFAULT_STREAM = True  # é»˜è®¤å¼€å¯æµå¼è¾“å‡ºï¼ŒFalseåˆ™ä¸ºéæµå¼

# DeepSeek-Chat APIé…ç½®ï¼ˆæ›¿æ¢ä¸ºä½ çš„æœ‰æ•ˆAPI Keyï¼‰
DEEPSEEK_API_KEY = "sk-8209e046f7234a128ecf9220030dd718"
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
DEEPSEEK_MODEL = "deepseek-chat"
DEEPSEEK_HEADERS = {
    "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
    "Content-Type": "application/json"
}

# Ollamaé…ç½®ï¼ˆæœ¬åœ°å¯åŠ¨OllamaæœåŠ¡åä½¿ç”¨ï¼‰
OLLAMA_API_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "qwen3:8b"

# ===================== å·¥å…·ç±»ï¼šå¤§æ¨¡å‹è°ƒç”¨å°è£…ï¼ˆæ”¯æŒæµå¼+éæµå¼ï¼‰ =====================
class LLMClient:
    """æ‰€æœ‰Agentç»Ÿä¸€è°ƒç”¨å¤§æ¨¡å‹çš„å°è£…ç±»ï¼Œæ”¯æŒæœ¬åœ°Ollamaå’Œè¿œç¨‹DeepSeekï¼Œå…¼å®¹æµå¼/éæµå¼è¾“å‡º"""
    @staticmethod
    def chat(messages: List[Dict], temperature: float = 0.7, stream: bool = DEFAULT_STREAM) -> str | Generator[str, None, None]:
        """
        ç»Ÿä¸€è°ƒç”¨å…¥å£ï¼šstream=Trueè¿”å›ç”Ÿæˆå™¨ï¼ˆæµå¼ï¼‰ï¼Œstream=Falseè¿”å›å­—ç¬¦ä¸²ï¼ˆéæµå¼ï¼‰
        :param messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        :param temperature: ç”Ÿæˆéšæœºæ€§
        :param stream: æ˜¯å¦å¼€å¯æµå¼è¾“å‡º
        :return: å­—ç¬¦ä¸²ï¼ˆéæµå¼ï¼‰æˆ–ç”Ÿæˆå™¨ï¼ˆæµå¼ï¼‰
        """
        if MODEL_TYPE == "deepseek":
            return LLMClient._chat_deepseek(messages, temperature, stream)
        elif MODEL_TYPE == "ollama":
            return LLMClient._chat_ollama(messages, temperature, stream)
        else:
            return f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹ï¼š{MODEL_TYPE}"
    
    @staticmethod
    def _chat_deepseek(messages: List[Dict], temperature: float = 0.7, stream: bool = False) -> str | Generator[str, None, None]:
        """DeepSeek æµå¼/éæµå¼è°ƒç”¨ï¼ˆSSE æ ¼å¼å“åº”ï¼‰"""
        payload = {
            "model": DEEPSEEK_MODEL,
            "messages": messages,
            "temperature": temperature,
            "stream": stream  # å…³é”®ï¼šæ§åˆ¶æ˜¯å¦æµå¼
        }
        try:
            # æµå¼è¯·æ±‚éœ€è®¾ç½® stream=Trueï¼Œé€è¡Œæ¥æ”¶å“åº”
            response = requests.post(DEEPSEEK_API_URL, headers=DEEPSEEK_HEADERS, json=payload, stream=stream, timeout=60)
            response.raise_for_status()

            # éæµå¼ï¼šç›´æ¥è¿”å›å®Œæ•´ç»“æœ
            if not stream:
                return response.json()["choices"][0]["message"]["content"].strip()
            
            # æµå¼ï¼šé€è¡Œè§£æ SSE å“åº”ï¼Œè¿”å›ç”Ÿæˆå™¨
            def generate():
                full_content = ""
                for line in response.iter_lines():
                    if line:
                        # è§£æ SSE æ ¼å¼ï¼šdata: {...}
                        line_str = line.decode("utf-8").strip()
                        if line_str.startswith("data: ") and line_str != "data: [DONE]":
                            chunk_data = json.loads(line_str[6:])  # å»æ‰ "data: " å‰ç¼€
                            chunk_content = chunk_data["choices"][0]["delta"].get("content", "")
                            if chunk_content:
                                full_content += chunk_content
                                yield chunk_content  # é€å—è¿”å›å†…å®¹
                return full_content
            return generate()

        except Exception as e:
            error_msg = f"DeepSeekè°ƒç”¨å¤±è´¥ï¼š{str(e)}"
            if stream:
                # æµå¼åœºæ™¯ä¸‹ï¼Œé”™è¯¯ä¿¡æ¯ä½œä¸ºç”Ÿæˆå™¨è¿”å›
                def error_gen():
                    yield error_msg
                return error_gen()
            return error_msg
    
    @staticmethod
    def _chat_ollama(messages: List[Dict], temperature: float = 0.7, stream: bool = False) -> str | Generator[str, None, None]:
        """Ollama æµå¼/éæµå¼è°ƒç”¨ï¼ˆé€è¡Œ JSON å“åº”ï¼‰"""
        payload = {
            "model": OLLAMA_MODEL,
            "messages": messages,
            "temperature": temperature,
            "stream": stream  # å…³é”®ï¼šæ§åˆ¶æ˜¯å¦æµå¼
        }
        try:
            response = requests.post(OLLAMA_API_URL, json=payload, stream=stream, timeout=60)
            response.raise_for_status()

            # éæµå¼ï¼šç›´æ¥è¿”å›å®Œæ•´ç»“æœ
            if not stream:
                return response.json()["message"]["content"].strip()
            
            # æµå¼ï¼šé€è¡Œè§£æ JSON å“åº”ï¼Œè¿”å›ç”Ÿæˆå™¨
            def generate():
                full_content = ""
                for line in response.iter_lines():
                    if line:
                        chunk_data = json.loads(line.decode("utf-8"))
                        chunk_content = chunk_data["message"].get("content", "")
                        if chunk_content:
                            full_content += chunk_content
                            yield chunk_content  # é€å—è¿”å›å†…å®¹
                return full_content
            return generate()

        except Exception as e:
            error_msg = f"Ollamaè°ƒç”¨å¤±è´¥ï¼š{str(e)}"
            if stream:
                def error_gen():
                    yield error_msg
                return error_gen()
            return error_msg

# ===================== å­Agentç±»ï¼ˆæ”¯æŒæµå¼æ‰§è¡Œï¼‰ =====================
class SubAgent:
    """
    å­Agentï¼šæ”¯æŒä¸“å±Promptå®šåˆ¶ï¼Œå…¼å®¹æµå¼/éæµå¼ä»»åŠ¡æ‰§è¡Œ
    :param agent_id: å­Agentå”¯ä¸€æ ‡è¯†
    :param role: å­Agentè§’è‰²
    :param ability_tags: èƒ½åŠ›æ ‡ç­¾
    :param prompt_template: å®šåˆ¶åŒ–Promptæ¨¡æ¿
    :param skills: è¯¥Agentå¯ç”¨çš„æŠ€èƒ½åˆ—è¡¨
    """
    def __init__(self, agent_id: str, role: str, ability_tags: List[str], prompt_template: str, skills: Optional[List[SkillMetadata]] = None):
        self.agent_id = agent_id
        self.role = role
        self.ability_tags = ability_tags
        self.prompt_template = prompt_template
        self.skills = skills or []
        self.llm = LLMClient()

    def execute_task(self, task: Dict, stream: bool = DEFAULT_STREAM) -> str:
        """
        æ‰§è¡Œå­ä»»åŠ¡ï¼šæ”¯æŒæµå¼è¾“å‡ºï¼ˆå®æ—¶æ‰“å°ï¼‰ï¼Œè¿”å›å®Œæ•´ç»“æœ
        :param task: å­ä»»åŠ¡å­—å…¸
        :param stream: æ˜¯å¦å¼€å¯æµå¼è¾“å‡º
        :return: å­ä»»åŠ¡å®Œæ•´æ‰§è¡Œç»“æœ
        """
        # æ›¿æ¢Promptæ¨¡æ¿å˜é‡
        final_prompt = self.prompt_template.format(
            task_name=task["name"],
            task_goal=task["goal"],
            task_input=task["input"],
            task_output=task["output"]
        )
        
        # æ·»åŠ æŠ€èƒ½ç³»ç»Ÿæç¤º
        skills_prompt = ""
        if self.skills:
            skills_prompt = format_skills_system_prompt(self.skills)
            final_prompt = skills_prompt + "\n" + final_prompt
        
        messages = [{"role": "user", "content": final_prompt}]
        
        # è°ƒç”¨å¤§æ¨¡å‹ï¼ˆæµå¼/éæµå¼ï¼‰
        result_gen = self.llm.chat(messages, temperature=0.6, stream=stream)
        
        # å¤„ç†æµå¼è¾“å‡ºï¼šå®æ—¶æ‰“å° + æ”¶é›†å®Œæ•´ç»“æœ
        full_result = ""
        if stream:
            print("å­ä»»åŠ¡è¾“å‡ºï¼š", end="", flush=True)
            for chunk in result_gen:
                print(chunk, end="", flush=True)
                full_result += chunk
            print()
        else:
            full_result = result_gen
            print(f"å­ä»»åŠ¡è¾“å‡ºï¼š{full_result[:60]}...")
        
        return full_result.strip()

# ===================== æ€»æ§Agentç±»ï¼ˆé€‚é…æµå¼è¾“å‡ºï¼‰ =====================
class MasterAgent:
    """æ€»æ§Agentï¼šé€šç”¨éœ€æ±‚æ‹†è§£ã€åŠ¨æ€ç”Ÿæˆå­Agentã€ä»»åŠ¡è°ƒåº¦ã€ç»“æœæ•´åˆï¼ˆæ”¯æŒæµå¼ï¼‰"""
    def __init__(self, skills_sources: Optional[List[str]] = None):
        self.llm = LLMClient()
        self.sub_agents: Dict[str, SubAgent] = {}
        self.task_results: Dict[str, str] = {}
        self.skills_sources = skills_sources or []
        self.all_skills: Dict[str, SkillMetadata] = {}
        
        # åŠ è½½æ‰€æœ‰ skills
        if self.skills_sources:
            self._load_skills()

    def register_sub_agent(self, sub_agent: SubAgent):
        """æ³¨å†Œå­Agentï¼ˆåŠ¨æ€ç”Ÿæˆåè‡ªåŠ¨è°ƒç”¨ï¼‰"""
        self.sub_agents[sub_agent.agent_id] = sub_agent

    def _load_skills(self):
        """åŠ è½½æ‰€æœ‰æŠ€èƒ½"""
        print("\n===== åŠ è½½ Skills =====")
        self.all_skills = list_skills(self.skills_sources)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.all_skills)} ä¸ªæŠ€èƒ½ï¼š")
        for skill_name, skill in self.all_skills.items():
            print(f"  - {skill_name}: {skill['description']}")

    def _parse_requirement(self, requirement: str) -> List[Dict]:
        """å¢å¼ºéœ€æ±‚æ‹†è§£ï¼šè¿”å›å­ä»»åŠ¡+è§’è‰²+èƒ½åŠ›+æ ¸å¿ƒè¦æ±‚"""
        parse_prompt = f"""
        ä½ æ˜¯ä¸“ä¸šéœ€æ±‚æ‹†è§£å¸ˆï¼Œå°†ç”¨æˆ·éœ€æ±‚æ‹†è§£ä¸ºã€å¯æ‰§è¡Œã€å¸¦ä¾èµ–ã€‘çš„å­ä»»åŠ¡ï¼Œä»…è¿”å›JSONæ•°ç»„ï¼Œæ— å…¶ä»–æ–‡å­—ã€‚
        å­ä»»åŠ¡å­—æ®µè¦æ±‚ï¼š
        1. task_idï¼šå”¯ä¸€æ ‡è¯†ï¼ˆå¦‚T001ï¼‰
        2. nameï¼šä»»åŠ¡åï¼ˆç®€æ´æ˜ç¡®ï¼‰
        3. goalï¼šä»»åŠ¡ç›®æ ‡ï¼ˆæ ¸å¿ƒç›®çš„ï¼‰
        4. inputï¼šä»»åŠ¡è¾“å…¥ï¼ˆæ‰€éœ€æ•°æ®/ä¿¡æ¯ï¼‰
        5. outputï¼šè¾“å‡ºè¦æ±‚ï¼ˆæ ¼å¼ã€å†…å®¹è§„èŒƒï¼‰
        6. dependenciesï¼šä¾èµ–ä»»åŠ¡IDï¼ˆæ— ä¾èµ–åˆ™ä¸ºç©ºæ•°ç»„[]ï¼‰
        7. tagsï¼šä»»åŠ¡æ ‡ç­¾ï¼ˆåŒ¹é…èƒ½åŠ›ï¼Œå¦‚["æ•°æ®å¤„ç†"]ï¼‰
        8. roleï¼šæ‰§è¡Œè¯¥ä»»åŠ¡çš„ä¸“å±å­Agentè§’è‰²ï¼ˆå¦‚"èµ„æ·±æ•°æ®åˆ†æå¸ˆ"ï¼‰
        9. core_requirementsï¼šè¯¥ä»»åŠ¡çš„æ ¸å¿ƒæ‰§è¡Œè¦æ±‚ï¼ˆå¦‚"æ•°æ®ç²¾å‡†ã€è¾“å‡ºå¯è§†åŒ–æè¿°"ï¼‰
        
        ç¤ºä¾‹ï¼š[{{
            "task_id":"T001",
            "name":"æ•°æ®é‡‡é›†",
            "goal":"è·å–2026å¹´1æœˆé”€å”®åŸå§‹æ•°æ®",
            "input":"é”€å”®Excelæ–‡ä»¶åœ°å€",
            "output":"æ¸…æ´—åçš„ç»“æ„åŒ–æ•°æ®é›†",
            "dependencies":[],
            "tags":["æ•°æ®å¤„ç†"],
            "role":"æ•°æ®é‡‡é›†ä¸“å‘˜",
            "core_requirements":"ç¡®ä¿æ•°æ®å®Œæ•´æ— ç¼ºå¤±ï¼Œæ¸…æ´—é‡å¤å€¼å’Œå¼‚å¸¸å€¼"
        }}]
        
        ç”¨æˆ·éœ€æ±‚ï¼š{requirement}
        """
        messages = [{"role": "user", "content": parse_prompt}]
        parse_result = self.llm.chat(messages, temperature=0.3, stream=False)  # æ‹†è§£éœ€æ±‚å›ºå®šéæµå¼ï¼Œä¿è¯ç»“æ„åŒ–
        try:
            return json.loads(parse_result)
        except:
            return [{
                "task_id":"T000",
                "name":"æ‹†è§£å¤±è´¥",
                "goal":"æ— ",
                "input":"æ— ",
                "output":"æ— ",
                "dependencies":[],
                "tags":["é€šç”¨"],
                "role":"é€šç”¨æ‰§è¡Œä¸“å®¶",
                "core_requirements":"æŒ‰è¦æ±‚å®ŒæˆåŸºç¡€ä»»åŠ¡ï¼Œè¾“å‡ºç®€æ´å‡†ç¡®"
            }]

    def _generate_dynamic_agent_prompt(self, task: Dict) -> str:
        """è‡ªåŠ¨ä¸ºå­ä»»åŠ¡ç”Ÿæˆä¸“å±Promptæ¨¡æ¿"""
        prompt_template = f"""
        ä½ æ˜¯{task['role']}ï¼Œä¸“ä¸šèƒ½åŠ›ï¼š{', '.join(task['tags'])}ã€‚
        æ ¸å¿ƒæ‰§è¡Œè¦æ±‚ï¼š{task['core_requirements']}ã€‚
  è¯·æ‰§è¡Œå­ä»»åŠ¡ï¼š{{task_name}}ï¼Œä»»åŠ¡ç›®æ ‡ï¼š{{task_goal}}ã€‚
  è¾“å…¥ä¿¡æ¯ï¼š{{task_input}}ï¼Œè¾“å‡ºè¦æ±‚ï¼š{{task_output}}ã€‚
  è¦æ±‚ï¼šä¸¥æ ¼éµå¾ªæ ¸å¿ƒæ‰§è¡Œè¦æ±‚ï¼Œè¾“å‡ºç²¾å‡†ã€ç¬¦åˆè§„èŒƒï¼Œæ— å†—ä½™å†…å®¹ã€‚
        """
        return prompt_template.strip()

    def _generate_dynamic_agents(self, tasks: List[Dict]):
        """åŠ¨æ€åˆ›å»ºå­Agentå¹¶è‡ªåŠ¨æ³¨å†Œï¼ˆåŒç±»ä»»åŠ¡å¤ç”¨ï¼‰"""
        print("\n===== åŠ¨æ€ç”Ÿæˆå­Agent =====")
        generated_agent_keys = set()
        
        for task in tasks:
            agent_key = f"{task['role']}_{'_'.join(task['tags'])}"
            if agent_key in generated_agent_keys:
                continue
            
            dynamic_prompt = self._generate_dynamic_agent_prompt(task)
            agent_id = f"A_{task['role'][:2].upper()}_{len(generated_agent_keys)+1:03d}"
            
            # æ ¹æ®ä»»åŠ¡æ ‡ç­¾åŒ¹é… skills
            matched_skills = match_skills_by_tags(task["tags"], self.all_skills)
            
            dynamic_agent = SubAgent(
                agent_id=agent_id,
                role=task['role'],
                ability_tags=task['tags'],
                prompt_template=dynamic_prompt,
                skills=matched_skills
            )
            self.register_sub_agent(dynamic_agent)
            generated_agent_keys.add(agent_key)
            
            skills_info = f"ï¼ŒåŒ…å« {len(matched_skills)} ä¸ªæŠ€èƒ½" if matched_skills else ""
            print(f"âœ… ç”Ÿæˆå­Agentï¼š{agent_id} - {task['role']}ï¼ˆæ ‡ç­¾ï¼š{task['tags']}{skills_info}ï¼‰")

    def _assign_agent_for_task(self, task: Dict) -> Optional[SubAgent]:
        """åŸºäºæ ‡ç­¾åŒ¹é…å­Agent"""
        for agent in self.sub_agents.values():
            if any(tag in agent.ability_tags for tag in task["tags"]):
                return agent
        for agent in self.sub_agents.values():
            if "é€šç”¨" in agent.ability_tags:
                return agent
        return None

    def _schedule_tasks(self, tasks: List[Dict], stream: bool = DEFAULT_STREAM) -> Dict[str, str]:
        """æŒ‰ä¾èµ–é¡ºåºæ‰§è¡Œä»»åŠ¡ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰"""
        remaining_tasks = tasks.copy()
        executed_tasks = set()
        results = {}

        print("\n===== æŒ‰ä¾èµ–é¡ºåºæ‰§è¡Œå­ä»»åŠ¡ =====")
        while remaining_tasks:
            executable_tasks = [t for t in remaining_tasks if all(dep in executed_tasks for dep in t["dependencies"])]
            if not executable_tasks:
                print("è­¦å‘Šï¼šå­˜åœ¨å¾ªç¯ä¾èµ–ï¼Œè°ƒåº¦ç»ˆæ­¢ï¼")
                break

            for task in executable_tasks:
                print(f"\nğŸ“Œ æ‰§è¡Œä»»åŠ¡ï¼š{task['task_id']} - {task['name']}")
                agent = self._assign_agent_for_task(task)
                if not agent:
                    result = "æ— å¯ç”¨å­Agent"
                    print(f"å­ä»»åŠ¡è¾“å‡ºï¼š{result}")
                else:
                    print(f"ğŸ¤– åˆ†é…å­Agentï¼š{agent.agent_id} - {agent.role}")
                    result = agent.execute_task(task, stream=stream)  # æµå¼æ‰§è¡Œå­ä»»åŠ¡
                results[task["task_id"]] = result
                executed_tasks.add(task["task_id"])
                time.sleep(0.5)  # çŸ­å»¶æ—¶ï¼Œæå‡äº¤äº’ä½“éªŒ

            remaining_tasks = [t for t in remaining_tasks if t["task_id"] not in executed_tasks]
        return results

    def _integrate_results(self, requirement: str, tasks: List[Dict], results: Dict[str, str], stream: bool = DEFAULT_STREAM) -> str:
        """ç»“æœæ•´åˆï¼ˆæ”¯æŒæµå¼è¾“å‡ºæœ€ç»ˆç»“æœï¼‰"""
        task_details = "\n".join([f"ä»»åŠ¡{t['task_id']}ï¼š{t['name']}\nç»“æœï¼š{results[t['task_id']]}" for t in tasks])
        integrate_prompt = f"""
        ä½ æ˜¯ç»“æœæ•´åˆä¸“å®¶ï¼Œæ ¹æ®åŸå§‹éœ€æ±‚å’Œå­ä»»åŠ¡ç»“æœï¼Œç”Ÿæˆå®Œæ•´ã€è¿è´¯çš„æœ€ç»ˆè¾“å‡ºï¼Œç›´æ¥è¾“å‡ºç»“æœï¼Œæ— é¢å¤–è§£é‡Šã€‚
        åŸå§‹éœ€æ±‚ï¼š{requirement}
        å­ä»»åŠ¡ç»“æœï¼š{task_details}
        """
        messages = [{"role": "user", "content": integrate_prompt}]
        
        # æµå¼è¾“å‡ºæœ€ç»ˆç»“æœ
        print("\n===== æ•´åˆæ‰€æœ‰ç»“æœï¼ˆæµå¼è¾“å‡ºï¼‰ =====")
        result_gen = self.llm.chat(messages, temperature=0.5, stream=stream)
        
        full_final_result = ""
        if stream:
            print("æœ€ç»ˆç»“æœï¼š", end="", flush=True)
            for chunk in result_gen:
                print(chunk, end="", flush=True)
                full_final_result += chunk
            print()
        else:
            full_final_result = result_gen
            print(f"æœ€ç»ˆç»“æœï¼š{full_final_result}")
        
        return full_final_result.strip()

    def run(self, requirement: str, stream: bool = DEFAULT_STREAM) -> str:
        """æ€»æ§ä¸»æµç¨‹ï¼ˆæ”¯æŒæµå¼ï¼‰"""
        print(f"===== æ€»æ§æ¥æ”¶éœ€æ±‚ï¼š{requirement} =====")
        # 1. æ‹†è§£éœ€æ±‚
        tasks = self._parse_requirement(requirement)
        if tasks[0]["task_id"] == "T000":
            return "éœ€æ±‚æ‹†è§£å¤±è´¥"
        print(f"æ‹†è§£å®Œæˆï¼Œå…±{len(tasks)}ä¸ªå­ä»»åŠ¡ï¼š")
        for t in tasks:
            print(f"- {t['task_id']}ï¼š{t['name']}ï¼ˆè§’è‰²ï¼š{t['role']}ï¼Œä¾èµ–ï¼š{t['dependencies']}ï¼‰")

        # 2. åŠ¨æ€ç”Ÿæˆå­Agent
        self._generate_dynamic_agents(tasks)

        # 3. è°ƒåº¦æ‰§è¡Œå­ä»»åŠ¡ï¼ˆæµå¼ï¼‰
        self.task_results = self._schedule_tasks(tasks, stream=stream)

        # 4. æ•´åˆç»“æœï¼ˆæµå¼ï¼‰
        final_result = self._integrate_results(requirement, tasks, self.task_results, stream=stream)
        return final_result

# ===================== è¿è¡Œç¤ºä¾‹ =====================
if __name__ == "__main__":
    # é…ç½® skills æºç›®å½•
    skills_sources = [
        "./skills",  # é¡¹ç›®çº§ skills
        # "~/.deepagents/skills",  # ç”¨æˆ·çº§ skillsï¼ˆå¯é€‰ï¼‰
    ]
    
    # åˆå§‹åŒ–æ€»æ§ï¼ˆä¼ å…¥ skills_sourcesï¼‰
    master = MasterAgent(skills_sources=skills_sources)

    # é€‰æ‹©éœ€æ±‚ç±»å‹
    print("è¯·é€‰æ‹©éœ€æ±‚ç±»å‹ï¼š")
    print("1. é¡¹ç›®ç±»ï¼šç­–åˆ’ä¸€åœºçº¿ä¸Šäº§å“å‘å¸ƒä¼šï¼ˆé¢å‘å¹´è½»ç”¨æˆ·ï¼Œå«å®£ä¼ ã€æµç¨‹ã€é¢„ç®—ï¼‰")
    print("2. å†…å®¹ç±»ï¼šå†™ä¸€ç¯‡AI Agentç§‘æ™®æ–‡ç« ï¼ˆ1500å­—ï¼Œå¤§ä¼—é˜…è¯»ï¼Œå«å®šä¹‰ã€åœºæ™¯ã€è¶‹åŠ¿ï¼‰")
    print("3. å·¥ä½œæµç±»ï¼šæ•´ç†2026å¹´1æœˆé”€å”®æ•°æ®ï¼Œç”Ÿæˆå«å›¾è¡¨çš„åˆ†ææŠ¥å‘Šï¼ˆä¸šç»©+é—®é¢˜+å»ºè®®ï¼‰")
    choice = input("è¾“å…¥é€‰æ‹©ï¼ˆ1/2/3ï¼‰ï¼š")

    if choice == "1":
        req = "ç­–åˆ’ä¸€åœºçº¿ä¸Šäº§å“å‘å¸ƒä¼šï¼Œé¢å‘å¹´è½»ç”¨æˆ·ç¾¤ä½“ï¼Œçªå‡ºäº§å“æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…å«å®£ä¼ æ–¹æ¡ˆã€æ‰§è¡Œæµç¨‹ã€é¢„ç®—æ ¸ç®—ã€é£é™©é¢„æ¡ˆ"
    elif choice == "2":
        req = "å†™ä¸€ç¯‡AI Agentåº”ç”¨çš„ç§‘æ™®æ–‡ç« ï¼Œ1500å­—å·¦å³ï¼Œé€‚åˆå¤§ä¼—é˜…è¯»ï¼Œé€šä¿—æ˜“æ‡‚ï¼Œè¦†ç›–å®šä¹‰ã€å®é™…åº”ç”¨åœºæ™¯ã€æœªæ¥å‘å±•è¶‹åŠ¿"
    elif choice == "3":
        req = "æ•´ç†2026å¹´1æœˆé”€å”®æ•°æ®ï¼ˆExcelæ ¼å¼ï¼‰ï¼Œç”Ÿæˆå«å¯è§†åŒ–å›¾è¡¨çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ä¸šç»©æ€»ç»“ã€é—®é¢˜åˆ†æã€ä¸‹æœˆä¼˜åŒ–å»ºè®®ï¼Œè¾“å‡ºPDFæ ¼å¼"
    else:
        req = "å†™ä¸€ç¯‡AI Agentåº”ç”¨çš„ç§‘æ™®æ–‡ç« ï¼ˆ1500å­—ï¼Œå¤§ä¼—é˜…è¯»ï¼‰"

    # æ‰§è¡Œæ€»æ§ï¼ˆé»˜è®¤æµå¼è¾“å‡ºï¼Œå¯æ‰‹åŠ¨æ”¹ä¸ºstream=Falseå…³é—­ï¼‰
    final_output = master.run(req, stream=DEFAULT_STREAM)